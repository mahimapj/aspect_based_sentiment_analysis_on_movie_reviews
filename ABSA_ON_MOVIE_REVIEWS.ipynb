{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6c584178193e4f0f8d8528fd6524be5b",
            "dcd3969ce6c24a6e8636a35b4384e85d",
            "2104cc6986dc4157927db3a9367561fe",
            "c8eaba166841497dbe3eb1c3bcf84b03",
            "f67b34ac47e34e4b9ff5f41e8cb93ee1",
            "65e9bfcc735e4ee685f3193761faf1a6",
            "68dfe51e3319464ab77b808c88ba4644",
            "8c5f3bc8e8f7452e9b4b3453d51777c4",
            "2e13d5cd375b492184af377a88492d7f",
            "cca4de6e719d49e78bcc67fffadaafe4",
            "20f251eee2634b22b1fb3acaf6899871",
            "44b23cff1a154b928c3eeb409cf3a6a5",
            "134d151983fe4b10afd3bd8154fb338f",
            "e4116c100367445fa22520e75a712200",
            "5e2eb18d9ad74242913f3bd730ccedd1",
            "0aca3261fab44df38de7c4994e0083b8",
            "055721b79f3b47e3b597cf92dcc9747b",
            "57ba9fe3be18415786897aa312b791a6",
            "9745e59a33f446638acdd7e88928e1b2",
            "8cd117028643442ab76147096b90e21f",
            "914e0549a6bc4eaea921484d4466b73e",
            "003b0ab9426545b589ac11a5e2cec408"
          ]
        },
        "collapsed": true,
        "id": "wcyaSnBoV4KK",
        "outputId": "bd017c06-5a5b-4b91-e76f-94a93fc1cb22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-c207dcc236fb>:103: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c584178193e4f0f8d8528fd6524be5b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-c207dcc236fb>:113: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss: 0.5213\n",
            "Epoch 2/2\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44b23cff1a154b928c3eeb409cf3a6a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Loss: 0.4194\n",
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://a0ee68ae8d1338a668.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://a0ee68ae8d1338a668.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "!pip install transformers[torch] gradio pandas matplotlib --quiet\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import gradio as gr\n",
        "from google.colab import drive\n",
        "from tqdm.notebook import tqdm\n",
        "ASPECTS = [\"acting\", \"dialogues\", \"storyline\", \"characters\", \"cinematography\", \"visual effects\", \"direction\"]\n",
        "ASPECT_CONFIG = {\n",
        "    \"acting\": {\"threshold\": 0.65, \"keywords\": [\"act\", \"perform\"]},\n",
        "    \"dialogues\": {\"threshold\": 0.6, \"keywords\": [\"dialogue\", \"script\"]},\n",
        "    \"storyline\": {\"threshold\": 0.55, \"keywords\": [\"story\", \"plot\"]},\n",
        "    \"characters\": {\"threshold\": 0.5, \"keywords\": [\"character\", \"role\"]},\n",
        "    \"cinematography\": {\"threshold\": 0.45, \"keywords\": [\"camera\", \"shot\"]},\n",
        "    \"visual effects\": {\"threshold\": 0.4, \"keywords\": [\"effect\", \"vfx\"]},\n",
        "    \"direction\": {\"threshold\": 0.6, \"keywords\": [\"director\", \"vision\"]}\n",
        "}\n",
        "BATCH_SIZE = 16\n",
        "MAX_LENGTH = 128\n",
        "EPOCHS = 2\n",
        "LEARNING_RATE = 4e-5\n",
        "\n",
        "def load_sample_data():\n",
        "    \"\"\"Load a smaller subset of data for faster processing\"\"\"\n",
        "    data = pd.read_csv(\"IMDB Dataset.csv\", nrows=2000)\n",
        "    data['sentiment'] = data['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "\n",
        "    for aspect in ASPECTS:\n",
        "        pattern = '|'.join(ASPECT_CONFIG[aspect][\"keywords\"])\n",
        "        data[aspect] = data['review'].str.contains(pattern, case=False).astype(int)\n",
        "\n",
        "    return data\n",
        "\n",
        "data = load_sample_data()\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "class FastAspectDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        }\n",
        "\n",
        "dataset = FastAspectDataset(\n",
        "    data['review'].values,\n",
        "    data[ASPECTS].values,\n",
        "    tokenizer,\n",
        "    MAX_LENGTH\n",
        ")\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=2)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=len(ASPECTS),\n",
        "    problem_type=\"multi_label_classification\"\n",
        ").to(device)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
        "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(\n",
        "                input_ids=batch['input_ids'].to(device),\n",
        "                attention_mask=batch['attention_mask'].to(device),\n",
        "                labels=batch['labels'].to(device)\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} Loss: {loss.item():.4f}\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_aspect_sentient(review):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(\n",
        "        review,\n",
        "        max_length=MAX_LENGTH,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_tensors='pt'\n",
        "    ).to(device)\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "    probs = torch.sigmoid(outputs.logits).cpu().numpy()[0]\n",
        "\n",
        "    results = {}\n",
        "    for i, aspect in enumerate(ASPECTS):\n",
        "        prob = probs[i]\n",
        "        threshold = ASPECT_CONFIG[aspect][\"threshold\"]\n",
        "        results[aspect] = {\n",
        "            'sentiment': 'Positive' if prob > threshold else 'Negative',\n",
        "            'confidence': float(prob if prob > threshold else 1 - prob),\n",
        "            'probability': float(prob)\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "def create_fast_plot(results):\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    aspects = list(results.keys())\n",
        "    confidences = [res['confidence'] for res in results.values()]\n",
        "    colors = ['green' if res['sentiment'] == 'Positive' else 'red' for res in results.values()]\n",
        "\n",
        "    bars = ax.bar(aspects, confidences, color=colors)\n",
        "    ax.set_title('Aspect Sentiment Analysis', pad=10)\n",
        "    ax.set_ylim(0, 1)\n",
        "\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{height:.2f}',\n",
        "                ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def analyze_review(review):\n",
        "    if not review.strip():\n",
        "        return \"Please enter a review\", None\n",
        "\n",
        "    results = predict_aspect_sentient(review)\n",
        "\n",
        "    text_output = \"Results:\\n\" + \"\\n\".join(\n",
        "        f\"{k}: {v['sentiment']} ({v['confidence']:.2f})\"\n",
        "        for k, v in results.items()\n",
        "    )\n",
        "\n",
        "    fig = create_fast_plot(results)\n",
        "    fig.savefig('temp_plot.png', bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "    return text_output, 'temp_plot.png'\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=analyze_review,\n",
        "    inputs=gr.Textbox(label=\"Review\", lines=5),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Analysis\"),\n",
        "        gr.Image(label=\"Visualization\")\n",
        "    ],\n",
        "    title=\"Fast Aspect Sentiment Analysis\",\n",
        "    examples=[\n",
        "        [\"Great acting and story but poor visual effects\"],\n",
        "        [\"Excellent direction makes up for weak dialogues\"]\n",
        "    ]\n",
        ")\n",
        "\n",
        "iface.launch(debug=True)"
      ]
    }
  ]
}